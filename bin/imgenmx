#!/usr/bin/env python

from datetime import date, datetime
from matplotlib import pyplot as plt
from mxnet import gluon
from mxnet import ndarray as nd
from mxnet.gluon import nn, utils
from mxnet import autograd
from PIL import Image

import argparse
import functools
import glob
import logging
import matplotlib as mpl
import tarfile
import matplotlib.image as mpimg
import mxnet as mx
import mxboard as mxb
import numpy as np
import os
import sys
import time
import toml

#
# AUX FUNCTIONS
#

def create_output_folder():

  ## Hourly folders
  output_dir = f'./{datetime.now():%Y-%m-%d--%H-00}/'
  if not os.path.exists(output_dir): os.makedirs(output_dir)

  return output_dir


def get_fs_path(xs):

  return functools.reduce(os.path.join, xs)


def transform(data, target_wd, target_ht):

  # resize to target_wd * target_ht
  data = mx.image.imresize(data, target_wd, target_ht)
  # transpose from (target_wd, target_ht, 3)
  # to (3, target_wd, target_ht)
  data = nd.transpose(data, (2,0,1))
  # normalize to [-1, 1]
  data = data.astype(np.float32)/127.5 - 1
  # if image is greyscale, repeat 3 times to get RGB image.
  if data.shape[0] == 1:
    data = nd.tile(data, (3, 1, 1))

  return data.reshape((1,) + data.shape)


def load_images(resolution: int, batch_size: int, image_input_folder: str) -> mx.io.io.NDArrayIter:

  logging.info('Calling this function loads the entire dataset into memory...')

  target_wd = resolution # TODO
  target_ht = resolution # TODO

  supported_image_types = ('.bmp', '.jpg', '.gif')

  img_list = []

  for path, _, fnames in os.walk(image_input_folder):
    for fname in fnames:
      if not fname.endswith(supported_image_types):
        continue
      img = os.path.join(path, fname)
      img_arr = mx.image.imread(img)
      img_arr = transform(img_arr, target_wd, target_ht)
      img_list.append(img_arr)

  train_data = mx.io.NDArrayIter(data=nd.concatenate(img_list), batch_size=batch_size)

  return train_data


def save_params(output_dir, netG, netD):

  try:

    ts = f'{datetime.now():%Y.%m.%d_%H.%M.%S}'

    netg_filename = f'{ts}.netg.params'
    netd_filename = f'{ts}.netd.params'

    netg_path = os.path.join(output_dir, netg_filename)
    netd_path = os.path.join(output_dir, netd_filename)

    netG.save_parameters(netg_path)
    netD.save_parameters(netd_path)

    return 'ok'

  except Exception as ex:
    logging.error('Error happened while trying to save params')
    logging.debug(ex)

    return 'err'


def load_params(input_dir, ctx, netG, netD):

  try:

    netgs = f"{input_dir}/*.netg.params"
    netds = f"{input_dir}/*.netd.params"

    last_netg = sorted(glob.glob(netgs))[-1]
    last_netd = sorted(glob.glob(netds))[-1]

    logging.info(f'Previous params are: {last_netg} and {last_netd}')

    netG.load_parameters(last_netg, ctx=ctx)
    netD.load_parameters(last_netd, ctx=ctx)

    logging.info(netG.collect_params('.*weight|.*bias'))
    logging.info(netD.collect_params('.*weight|.*bias'))

    return (netG, netD)

  except Exception as ex:
    logging.error('Error happened while trying to load params')
    logging.debug(ex)

    return 'err'


def fill_buf(buf, num_images, img, shape):

  width   = buf.shape[0]/shape[1]
  height  = buf.shape[1]/shape[0]

  img_width = int(num_images%width)*shape[0]
  img_hight = int(num_images/height)*shape[1]

  buf[img_hight:img_hight+shape[1], img_width:img_width+shape[0], :] = img


def visualize(fake, real):
    #64x3x64x64 to 64x64x64x3
    fake = fake.transpose((0, 2, 3, 1))
    #Pixel values from 0-255
    fake = np.clip((fake+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)
    #Repeat for real image
    real = real.transpose((0, 2, 3, 1))
    real = np.clip((real+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)

    #Create buffer array that will hold all the images in the batch
    #Fill the buffer so to arrange all images in the batch onto the buffer array
    n = np.ceil(np.sqrt(fake.shape[0]))
    fbuff = np.zeros((int(n*fake.shape[1]), int(n*fake.shape[2]), int(fake.shape[3])), dtype=np.uint8)
    for i, img in enumerate(fake):
        fill_buf(fbuff, i, img, fake.shape[1:3])
    rbuff = np.zeros((int(n*real.shape[1]), int(n*real.shape[2]), int(real.shape[3])), dtype=np.uint8)
    for i, img in enumerate(real):
        fill_buf(rbuff, i, img, real.shape[1:3])

    #Create a matplotlib figure with two subplots: one for the real and the other for the fake
    #fill each plot with the buffer array, which creates the image
    ts = f'{datetime.now():%Y.%m.%d_%H.%M.%S}'
    plt.imsave(f'{ts}.real.png', rbuff)
    plt.imsave(f'{ts}.fake.png', fbuff)


def uploading_trained_models_to_s3():

  # get last files
  # upload files

  return 'ok'


#
# MODELS
#


def create_generator():

  # build the generator
  nc = 3
  ngf = 64
  netG = nn.Sequential()

  with netG.name_scope():
      # input is Z, going into a convolution
      netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 4 x 4
      netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 8 x 8
      netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 16 x 16
      netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 32 x 32
      netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))
      netG.add(nn.Activation('tanh'))
      # state size. (nc) x 64 x 64

  return netG


def create_discriminator():

  # build the discriminator
  ndf = 64
  netD = nn.Sequential()

  with netD.name_scope():
      # input is (nc) x 64 x 64
      netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 32 x 32
      netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 16 x 16
      netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 8 x 8
      netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 4 x 4
      netD.add(nn.Conv2D(1, 4, 1, 0, use_bias=False))

  return netD


def create_trainers(ctx, lr, beta1, netG, netD):

  # loss
  loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()

  # initialize the generator and the discriminator
  netG.initialize(mx.init.Normal(0.02), ctx=ctx)
  netD.initialize(mx.init.Normal(0.02), ctx=ctx)

  # trainer for the generator and the discriminator
  trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})
  trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})

  return (trainerG, trainerD, loss)


def facc(label, pred):

  pred = pred.ravel()
  label = label.ravel()

  return ((pred > 0.5) == label).mean()


def update_d_network(data, latent_z, netG, netD, trainerD, loss, real_label, fake_label, metric, batch):

  ############################
  # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
  ###########################
  errD = None
  with autograd.record():
      # train with real image
      output = netD(data).reshape((-1, 1))
      errD_real = loss(output, real_label)
      metric.update([real_label,], [output,])

      # train with fake image
      fake = netG(latent_z)
      output = netD(fake.detach()).reshape((-1, 1))
      errD_fake = loss(output, fake_label)
      errD = errD_real + errD_fake
      errD.backward()
      metric.update([fake_label,], [output,])

  trainerD.step(batch.data[0].shape[0])

  return errD

def update_g_network(latent_z, netG, netD, trainerG, loss, real_label, batch):

  ############################
  # (2) Update G network: maximize log(D(G(z)))
  ###########################
  errG = None
  with autograd.record():
      fake = netG(latent_z)
      output = netD(fake).reshape((-1, 1))
      errG = loss(output, real_label)
      errG.backward()

  trainerG.step(batch.data[0].shape[0])

  return errG


def start_training(resolution, epochs, batch_size, gpu, image_input_folder, nets_input_folder):

  logging.info(f'resolution: {resolution}, epochs: {epochs}, batch_size: {batch_size}, gpu: {gpu}, image_input_folder:{image_input_folder}, nets_input_folder:{nets_input_folder}')

  # if GPU acceleration is enabled
  ctx = mx.gpu() if gpu else mx.cpu()

  # learning parameters (could be in config)
  lr = 0.0002
  beta1 = 0.5
  latent_z_size = 100
  # saving pictures and checkpoint
  output_dir = create_output_folder()

  #
  # LOADING DATA
  #
  train_data = load_images(resolution, batch_size, image_input_folder)
  logging.info(f'Training data loaded. {train_data}')

  #
  # GENERATOR & DISCRIMINATOR
  #
  netG = create_generator()
  netD = create_discriminator()

  logging.info(f'Generator: {netG}')
  logging.info(f'Discriminator: {netD}')


  #
  # CREATE TRAINERS
  #

  (trainerG, trainerD, loss) = create_trainers(ctx, lr, beta1, netG, netD)
  logging.info(f'Generator trainer: {trainerG}')
  logging.info(f'Discriminator trainer: {trainerD}')
  logging.info(f'Loss: {loss}')

  #
  # LOADING PREVIOUS TRAINING
  #

  if nets_input_folder:
    logging.info('Loading previous training state...')
    (netG, netD) = load_params(nets_input_folder, ctx, netG, netD)


  #
  # Start / continue training
  #

  real_label = nd.ones((batch_size,), ctx=ctx)
  fake_label = nd.zeros((batch_size,),ctx=ctx)

  metric = mx.metric.CustomMetric(facc)

  debug = False

  sw = None
  if debug:
    sw = mxb.SummaryWriter(logdir='mxb', flush_secs = 5)
    sw.add_graph(netG)
    sw.add_graph(netD)

  for epoch in range(epochs):
    tic = time.time()
    btic = time.time()
    train_data.reset()
    iter = 0
    batch = None
    for batch in train_data:

      data = batch.data[0].as_in_context(ctx)

      latent_z = mx.nd.random_normal(0, 1, shape=(batch_size, latent_z_size, 1, 1), ctx=ctx)

      errD = update_d_network(data, latent_z, netG, netD, trainerD, loss, real_label, fake_label, metric, batch)
      errG = update_g_network(latent_z, netG, netD, trainerG, loss, real_label, batch)

      # Print log infomation every ten batches
      if iter % 20 == 0:
          name, acc = metric.get()
          logging.info(f'speed: {batch_size / (time.time() - btic)} samples/s')
          logging.info(f'discriminator loss = {nd.mean(errD).asscalar()}, generator loss = {nd.mean(errG).asscalar()}, binary training acc = {acc} at iter {iter} epoch {epoch}')

      iter = iter + 1
      btic = time.time()

    name, acc = metric.get()
    metric.reset()
    logging.info(f'binary training acc at epoch {epoch}: {name}={acc}')
    logging.info(f'time: {time.time() - tic}')

    if epoch % 20 == 0:
      logging.info(f'Saving models: {save_params(output_dir, netG, netD)}')

    # getting some fakes
    outG = generator.get_outputs()
    visualize(outG[0].asnumpy(), batch.data[0].asnumpy())
    # end for epoch

  logging.info(f'DCGAN has finished training....')
  logging.info(f'Uploading to S3: {uploading_trained_models_to_s3()}')

  return 'ok'


#
# CLI
#


def train_cli(args, config):
  start_training(
    args.resolution,
    args.epochs,
    args.batch_size,
    args.gpu,
    args.image_input_folder,
    args.nets_input_folder
  )


def str2bool(v):
  if isinstance(v, bool):
    return v
  if v.lower() in ('yes', 'true', 't', 'y', '1'):
    return True
  elif v.lower() in ('no', 'false', 'f', 'n', '0'):
    return False
  else:
    raise argparse.ArgumentTypeError('Boolean value expected.')


def noop(args=None, config=None):
  logging.error('Not implemented function is called')


def args_switch(args, config):
  fn = switcher.get(args.func, noop)
  logging.info('fn: %s', fn)
  return fn(args, config)


switcher = {
  'train': train_cli,
}


def main():

  try:

    exe_path = os.path.dirname(os.path.realpath(sys.argv[0]))

    config = toml.load(os.path.join(exe_path, '..', 'config', 'imgen.toml'))

    log_folder_relative = config.get('main', {}).get('log_folder', 'logs')
    log_folder_absolute = os.path.join(exe_path, '..', log_folder_relative)

    if not os.path.isdir(log_folder_absolute):
      os.makedirs(log_folder_absolute, 0o750, exist_ok=True)

    today = str(date.today())

    log_handlers = []
    log_handlers.append(logging.FileHandler("{0}/{1}.{2}.log".format(log_folder_absolute, today, os.getpid())))
    log_handlers.append(logging.StreamHandler(sys.stdout))
    logging.basicConfig(
      level=logging.INFO,
      format=config.get('main', {}).get('log_pattern', '%(asctime)s %(levelname)-4s %(message)s'),
      datefmt=config.get('main', {}).get('log_date_fmt', '%Y-%m-%d %H:%M:%S'),
      handlers=log_handlers)

    parser = argparse.ArgumentParser(prog='imgen')

    subparsers = parser.add_subparsers()

    train = subparsers.add_parser('train')

    train.add_argument_group('train', '')
    train.set_defaults(func='train')
    train.add_argument('--image-input-folder', action='store', required=True)
    train.add_argument('--resolution', action='store', type=int, required=True)
    train.add_argument('--epochs', action='store', type=int, required=True)
    train.add_argument('--batch-size', action='store', type=int, required=True)
    train.add_argument('--gpu', action='store', type=str2bool, default=False, required=True)
    train.add_argument('--nets-input-folder', action='store', required=False, default=None)

    args = parser.parse_args()

    logging.info('ARGS: %s', args)

    if not any(vars(args).values()):
      logging.error("No parameter were passed")
      parser.print_help()
      exit(1)
    else:
      args_switch(args, config)

  except KeyboardInterrupt:
    logging.info("Ctrl+c was pressed, exiting...")
    exit(0)
  except Exception as e:
    logging.error('Exception caught in main')
    logging.exception('Exception caught: %s', e)
    exit(1)
  finally:
    logging.info("Quitting...")

if __name__ == '__main__':
  exit(main())
