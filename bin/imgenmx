#!/usr/bin/env python

from datetime import date
from matplotlib import pyplot as plt
from mxnet import gluon
from mxnet import ndarray as nd
from mxnet.gluon import nn, utils
from mxnet import autograd
from PIL import Image

import argparse
import datetime
import functools
import logging
import matplotlib as mpl
import tarfile
import matplotlib.image as mpimg
import mxnet as mx
import numpy as np
import os
import sys
import time
import toml

#
# AUX FUNCTIONS
#

def create_output_folder():

  ## Hourly folders
  output_dir = f'./{datetime.datetime.now():%Y-%m-%d--%H-00}/'
  if not os.path.exists(output_dir): os.makedirs(output_dir)

  return output_dir


def get_fs_path(xs):

  return functools.reduce(os.path.join, xs)


def transform(data, target_wd, target_ht):

  # resize to target_wd * target_ht
  data = mx.image.imresize(data, target_wd, target_ht)
  # transpose from (target_wd, target_ht, 3)
  # to (3, target_wd, target_ht)
  data = nd.transpose(data, (2,0,1))
  # normalize to [-1, 1]
  data = data.astype(np.float32)/127.5 - 1
  # if image is greyscale, repeat 3 times to get RGB image.
  if data.shape[0] == 1:
    data = nd.tile(data, (3, 1, 1))

  return data.reshape((1,) + data.shape)


def load_images(resolution: int, batch_size: int, image_input_folder: str) -> mx.io.io.NDArrayIter:

  logging.info('Calling this function loads the entire dataset into memory...')

  target_wd = resolution # TODO
  target_ht = resolution # TODO

  supported_image_types = ('.bmp', '.jpg', '.gif')

  img_list = []

  for path, _, fnames in os.walk(image_input_folder):
    for fname in fnames:
      if not fname.endswith(supported_image_types):
        continue
      img = os.path.join(path, fname)
      img_arr = mx.image.imread(img)
      img_arr = transform(img_arr, target_wd, target_ht)
      img_list.append(img_arr)

  train_data = mx.io.NDArrayIter(data=nd.concatenate(img_list), batch_size=batch_size)

  return train_data


def visualize(img_arr):
  plt.imshow(((img_arr.asnumpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))
  plt.axis('off')

def miez(netG):

  num_image = 8
  for i in range(num_image):
    latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)
    img = netG(latent_z)
    plt.subplot(2,4,i+1)
    visualize(img[0])
  plt.show()




def create_generator():

  # build the generator
  nc = 3
  ngf = 64
  netG = nn.Sequential()

  with netG.name_scope():
      # input is Z, going into a convolution
      netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 4 x 4
      netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 8 x 8
      netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 16 x 16
      netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))
      netG.add(nn.BatchNorm())
      netG.add(nn.Activation('relu'))
      # state size. (ngf*8) x 32 x 32
      netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))
      netG.add(nn.Activation('tanh'))
      # state size. (nc) x 64 x 64

  return netG


def create_discriminator():

  # build the discriminator
  ndf = 64
  netD = nn.Sequential()

  with netD.name_scope():
      # input is (nc) x 64 x 64
      netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 32 x 32
      netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 16 x 16
      netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 8 x 8
      netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))
      netD.add(nn.BatchNorm())
      netD.add(nn.LeakyReLU(0.2))
      # state size. (ndf) x 4 x 4
      netD.add(nn.Conv2D(1, 4, 1, 0, use_bias=False))

  return netD


def create_trainers(ctx, lr, beta1, netG, netD):

  # loss
  loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()

  # initialize the generator and the discriminator
  netG.initialize(mx.init.Normal(0.02), ctx=ctx)
  netD.initialize(mx.init.Normal(0.02), ctx=ctx)

  # trainer for the generator and the discriminator
  trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})
  trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})

  return (trainerG, trainerD, loss)


def facc(label, pred):

  pred = pred.ravel()
  label = label.ravel()

  return ((pred > 0.5) == label).mean()


def update_d_network(data, latent_z, netG, netD, trainerD, loss, real_label, fake_label, metric, batch):

  ############################
  # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
  ###########################
  errD = None
  with autograd.record():
      # train with real image
      output = netD(data).reshape((-1, 1))
      errD_real = loss(output, real_label)
      metric.update([real_label,], [output,])

      # train with fake image
      fake = netG(latent_z)
      output = netD(fake.detach()).reshape((-1, 1))
      errD_fake = loss(output, fake_label)
      errD = errD_real + errD_fake
      errD.backward()
      metric.update([fake_label,], [output,])

  trainerD.step(batch.data[0].shape[0])

  return errD

def update_g_network(latent_z, netG, netD, trainerG, loss, real_label, batch):

  ############################
  # (2) Update G network: maximize log(D(G(z)))
  ###########################
  errG = None
  with autograd.record():
      fake = netG(latent_z)
      output = netD(fake).reshape((-1, 1))
      errG = loss(output, real_label)
      errG.backward()

  trainerG.step(batch.data[0].shape[0])

  return errG


def start_training(resolution, epochs, batch_size, gpu, image_input_folder):

  # if GPU acceleration is enabled
  ctx = mx.gpu() if gpu else mx.cpu()
  # learning parameters (could be in config)
  lr = 0.0002
  beta1 = 0.5
  latent_z_size = 100
  # saving pictures and checkpoint
  output_dir = create_output_folder()

  #
  # LOADING DATA
  #
  train_data = load_images(resolution, batch_size, image_input_folder)
  logging.info(f'Training data loaded. {train_data}')


  #
  # GENERATOR & DISCRIMINATOR
  #
  netG = create_generator()
  netD = create_discriminator()

  logging.info(f'Generator: {netG}')
  logging.info(f'Discriminator: {netD}')

  #
  # CREATE TRAINERS
  #

  (trainerG, trainerD, loss) = create_trainers(ctx, lr, beta1, netG, netD)
  logging.info(f'Generator trainer: {trainerG}')
  logging.info(f'Discriminator trainer: {trainerD}')
  logging.info(f'Loss: {loss}')


  #
  # Start training
  #

  real_label = nd.ones((batch_size,), ctx=ctx)
  fake_label = nd.zeros((batch_size,),ctx=ctx)

  metric = mx.metric.CustomMetric(facc)

  for epoch in range(epochs):
      tic = time.time()
      btic = time.time()
      train_data.reset()
      iter = 0
      for batch in train_data:

          data = batch.data[0].as_in_context(ctx)

          latent_z = mx.nd.random_normal(0, 1, shape=(batch_size, latent_z_size, 1, 1), ctx=ctx)

          errD = update_d_network(data, latent_z, netG, netD, trainerD, loss, real_label, fake_label, metric, batch)
          errG = update_g_network(latent_z, netG, netD, trainerG, loss, real_label, batch)

          # Print log infomation every ten batches
          if iter % 10 == 0:
              name, acc = metric.get()
              logging.info(f'speed: {batch_size / (time.time() - btic)} samples/s')
              logging.info(f'discriminator loss = {nd.mean(errD).asscalar()}, generator loss = {nd.mean(errG).asscalar()}, binary training acc = {acc} at iter {iter} epoch {epoch}')
              miez(netG)
              # latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)
              # raw_gen = netG(latent_z)
              # img_gen = ((raw_gen + 1.0) * 127.5).astype(np.uint8)
              # img_gen2 = ((raw_gen.asnumpy() + 1.0) * 127.5).astype(np.uint8)
              # img = Image.fromarray(img_gen2)
              # img_path = f'{output_dir}/image_at_epoch_{epoch:04d}.png'
              # logging.info(f'Saving image to {img_path}')
              # img.save(img_path)

          iter = iter + 1
          btic = time.time()

      name, acc = metric.get()
      metric.reset()
      logging.info(f'binary training acc at epoch {epoch}: {name}={acc}')
      logging.info(f'time: {time.time() - tic}')

  logging.info(f'DCGAN has finished training....')

  return 'ok'


#
# CLI
#


def train_cli(args, config):
  start_training(
    args.resolution,
    args.epochs,
    args.batch_size,
    args.gpu,
    args.image_input_folder
  )


def str2bool(v):
  if isinstance(v, bool):
    return v
  if v.lower() in ('yes', 'true', 't', 'y', '1'):
    return True
  elif v.lower() in ('no', 'false', 'f', 'n', '0'):
    return False
  else:
    raise argparse.ArgumentTypeError('Boolean value expected.')


def noop(args=None, config=None):
  logging.error('Not implemented function is called')


def args_switch(args, config):
  fn = switcher.get(args.func, noop)
  logging.info('fn: %s', fn)
  return fn(args, config)


switcher = {
  'train': train_cli,
}


def main():

  try:

    exe_path = os.path.dirname(os.path.realpath(sys.argv[0]))

    config = toml.load(os.path.join(exe_path, '..', 'config', 'imgen.toml'))

    log_folder_relative = config.get('main', {}).get('log_folder', 'logs')
    log_folder_absolute = os.path.join(exe_path, '..', log_folder_relative)

    if not os.path.isdir(log_folder_absolute):
      os.makedirs(log_folder_absolute, 0o750, exist_ok=True)

    today = str(date.today())

    log_handlers = []
    log_handlers.append(logging.FileHandler("{0}/{1}.{2}.log".format(log_folder_absolute, today, os.getpid())))
    log_handlers.append(logging.StreamHandler(sys.stdout))
    logging.basicConfig(
      level=logging.INFO,
      format=config.get('main', {}).get('log_pattern', '%(asctime)s %(levelname)-4s %(message)s'),
      datefmt=config.get('main', {}).get('log_date_fmt', '%Y-%m-%d %H:%M:%S'),
      handlers=log_handlers)

    parser = argparse.ArgumentParser(prog='imgen')

    subparsers = parser.add_subparsers()

    train = subparsers.add_parser('train')

    train.add_argument_group('train', '')
    train.set_defaults(func='train')
    train.add_argument('--image-input-folder', action='store', required=True)
    train.add_argument('--resolution', action='store', type=int, required=True)
    train.add_argument('--epochs', action='store', type=int, required=True)
    train.add_argument('--batch-size', action='store', type=int, required=True)
    train.add_argument('--gpu', action='store', type=str2bool, default=False, required=True)

    args = parser.parse_args()

    logging.info('ARGS: %s', args)

    if not any(vars(args).values()):
      logging.error("No parameter were passed")
      parser.print_help()
      exit(1)
    else:
      args_switch(args, config)

  except KeyboardInterrupt:
    logging.info("Ctrl+c was pressed, exiting...")
    exit(0)
  except Exception as e:
    logging.error('Exception caught in main')
    logging.exception('Exception caught: %s', e)
    exit(1)
  finally:
    logging.info("Quitting...")

if __name__ == '__main__':
  exit(main())
